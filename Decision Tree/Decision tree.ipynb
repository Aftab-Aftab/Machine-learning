{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aZ_3Q-1UpMGr"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import math, os, joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier as SKDecisionTree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_conAOsEqqbN"
      },
      "outputs": [],
      "source": [
        "def load_adult():\n",
        "    try:\n",
        "        from ucimlrepo import fetch_ucirepo\n",
        "        adult = fetch_ucirepo(id=2)\n",
        "        X = adult.data.features\n",
        "        y = adult.data.targets\n",
        "        if isinstance(y, pd.DataFrame):\n",
        "            y = y.iloc[:,0]\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"ucimlrepo unavailable or fetch failed:\", e)\n",
        "        try:\n",
        "            from sklearn.datasets import fetch_openml\n",
        "            data = fetch_openml(name='adult', version=2, as_frame=True)\n",
        "            X = data.data\n",
        "            y = data.target\n",
        "            return X, y\n",
        "        except Exception as e2:\n",
        "            print(\"OpenML fallback failed:\", e2)\n",
        "            raise RuntimeError(\"Could not fetch Adult dataset. Please run in an environment with internet or provide the data file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "64Bf_qCkrAxm"
      },
      "outputs": [],
      "source": [
        "class TreeNode:\n",
        "    def __init__(self, depth=0):\n",
        "        self.depth = depth\n",
        "        self.is_leaf = False\n",
        "        self.prediction = None\n",
        "        self.feature_index = None\n",
        "        self.threshold = None\n",
        "        self.is_categorical = False\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.class_counts = None\n",
        "        self.samples = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ffo14QYRrB1m"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeFromScratch:\n",
        "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2, min_impurity_decrease=1e-7):\n",
        "        assert criterion in ('gini', 'entropy')\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.root = None\n",
        "\n",
        "    def _impurity(self, y):\n",
        "        counts = np.bincount(y, minlength=1)\n",
        "        probs = counts[counts>0] / y.size\n",
        "        if self.criterion == 'gini':\n",
        "            return 1.0 - np.sum(probs**2)\n",
        "        else:\n",
        "            return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "    def _weighted_impurity(self, y_left, y_right):\n",
        "        n = y_left.size + y_right.size\n",
        "        return (y_left.size/n)*self._impurity(y_left) + (y_right.size/n)*self._impurity(y_right)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples < self.min_samples_split:\n",
        "            return None\n",
        "        parent_impurity = self._impurity(y)\n",
        "        best_gain = 0.0\n",
        "        best = None\n",
        "\n",
        "        for feat in range(n_features):\n",
        "            values = X[:, feat]\n",
        "            unique_vals = np.unique(values)\n",
        "            if unique_vals.size == 1:\n",
        "                continue\n",
        "\n",
        "            is_categorical = (unique_vals.size <= 15 and np.all(unique_vals == np.floor(unique_vals)))\n",
        "            if is_categorical:\n",
        "                for val in unique_vals:\n",
        "                    left_idx = np.where(values == val)[0]\n",
        "                    right_idx = np.where(values != val)[0]\n",
        "                    if left_idx.size == 0 or right_idx.size == 0:\n",
        "                        continue\n",
        "                    impurity = self._weighted_impurity(y[left_idx], y[right_idx])\n",
        "                    gain = parent_impurity - impurity\n",
        "                    if gain > best_gain + 1e-12:\n",
        "                        best_gain = gain\n",
        "                        best = (feat, val, True, left_idx, right_idx)\n",
        "            else:\n",
        "                sorted_idx = np.argsort(values)\n",
        "                sorted_vals = values[sorted_idx]\n",
        "                sorted_y = y[sorted_idx]\n",
        "                for i in range(1, n_samples):\n",
        "                    if sorted_vals[i] == sorted_vals[i-1]:\n",
        "                        continue\n",
        "                    thresh = (sorted_vals[i] + sorted_vals[i-1]) / 2.0\n",
        "                    left_idx = np.where(values <= thresh)[0]\n",
        "                    right_idx = np.where(values > thresh)[0]\n",
        "                    if left_idx.size == 0 or right_idx.size == 0:\n",
        "                        continue\n",
        "                    impurity = self._weighted_impurity(y[left_idx], y[right_idx])\n",
        "                    gain = parent_impurity - impurity\n",
        "                    if gain > best_gain + 1e-12:\n",
        "                        best_gain = gain\n",
        "                        best = (feat, thresh, False, left_idx, right_idx)\n",
        "\n",
        "        if best is None or best_gain < self.min_impurity_decrease:\n",
        "            return None\n",
        "        feat, thresh, is_cat, left_idx, right_idx = best\n",
        "        return feat, thresh, is_cat, left_idx, right_idx\n",
        "\n",
        "    def _build(self, X, y, depth=0):\n",
        "        node = TreeNode(depth=depth)\n",
        "        node.samples = y.size\n",
        "        counts = np.bincount(y, minlength=2)\n",
        "        node.class_counts = counts\n",
        "        node.prediction = np.argmax(counts)\n",
        "        # stopping conditions\n",
        "        if np.unique(y).size == 1:\n",
        "            node.is_leaf = True\n",
        "            return node\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            node.is_leaf = True\n",
        "            return node\n",
        "        if y.size < self.min_samples_split:\n",
        "            node.is_leaf = True\n",
        "            return node\n",
        "\n",
        "        split = self._best_split(X, y)\n",
        "        if split is None:\n",
        "            node.is_leaf = True\n",
        "            return node\n",
        "\n",
        "        feat, thresh, is_cat, left_idx, right_idx = split\n",
        "        node.feature_index = feat\n",
        "        node.threshold = thresh\n",
        "        node.is_categorical = is_cat\n",
        "\n",
        "        node.left = self._build(X[left_idx], y[left_idx], depth+1)\n",
        "        node.right = self._build(X[right_idx], y[right_idx], depth+1)\n",
        "        return node\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_classes_ = len(np.unique(y))\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "        self.root = self._build(X, y, depth=0)\n",
        "        return self\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        while not node.is_leaf:\n",
        "            if node.is_categorical:\n",
        "                if x[node.feature_index] == node.threshold:\n",
        "                    node = node.left\n",
        "                else:\n",
        "                    node = node.right\n",
        "            else:\n",
        "                if x[node.feature_index] <= node.threshold:\n",
        "                    node = node.left\n",
        "                else:\n",
        "                    node = node.right\n",
        "        return node.prediction\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_one(x, self.root) for x in X])\n",
        "\n",
        "    def post_prune(self, X_val, y_val):\n",
        "        \"\"\"\n",
        "        Reduced error pruning:\n",
        "        - consider every internal node; if replacing it by leaf (majority class) does not reduce val accuracy, keep the replacement.\n",
        "        - repeat until no improvement\n",
        "        \"\"\"\n",
        "        improved = True\n",
        "        while improved:\n",
        "            improved = False\n",
        "            current_acc = accuracy_score(y_val, self.predict(X_val))\n",
        "            # get list of internal nodes\n",
        "            stack = [(self.root, None, None)]\n",
        "            candidates = []\n",
        "            while stack:\n",
        "                node, parent, is_left = stack.pop()\n",
        "                if node is None or node.is_leaf:\n",
        "                    continue\n",
        "                candidates.append((node, parent, is_left))\n",
        "                stack.append((node.left, node, True))\n",
        "                stack.append((node.right, node, False))\n",
        "\n",
        "            best_candidate = None\n",
        "            for node, parent, is_left in candidates:\n",
        "                # simulate pruning\n",
        "                # backup\n",
        "                if parent is None:\n",
        "                    saved_root = self.root\n",
        "                else:\n",
        "                    saved_child = parent.left if is_left else parent.right\n",
        "                # create leaf\n",
        "                new_leaf = TreeNode(depth=node.depth)\n",
        "                new_leaf.is_leaf = True\n",
        "                new_leaf.prediction = int(np.argmax(node.class_counts))\n",
        "                # attach\n",
        "                if parent is None:\n",
        "                    self.root = new_leaf\n",
        "                else:\n",
        "                    if is_left:\n",
        "                        parent.left = new_leaf\n",
        "                    else:\n",
        "                        parent.right = new_leaf\n",
        "                # evaluate\n",
        "                acc = accuracy_score(y_val, self.predict(X_val))\n",
        "                # restore\n",
        "                if parent is None:\n",
        "                    self.root = saved_root\n",
        "                else:\n",
        "                    if is_left:\n",
        "                        parent.left = saved_child\n",
        "                    else:\n",
        "                        parent.right = saved_child\n",
        "                if acc >= current_acc - 1e-12:\n",
        "                    current_acc = acc\n",
        "                    best_candidate = (node, parent, is_left)\n",
        "\n",
        "            if best_candidate is not None:\n",
        "                node, parent, is_left = best_candidate\n",
        "                leaf = TreeNode(depth=node.depth)\n",
        "                leaf.is_leaf = True\n",
        "                leaf.prediction = int(np.argmax(node.class_counts))\n",
        "                if parent is None:\n",
        "                    self.root = leaf\n",
        "                else:\n",
        "                    if is_left:\n",
        "                        parent.left = leaf\n",
        "                    else:\n",
        "                        parent.right = leaf\n",
        "                improved = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-ChDVyb7rL3U"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X, y):\n",
        "    preds = model.predict(X)\n",
        "    acc = accuracy_score(y, preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y, preds, average='binary', zero_division=0)\n",
        "    cm = confusion_matrix(y, preds)\n",
        "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"confusion_matrix\": cm}\n",
        "\n",
        "def main():\n",
        "    X_raw, y_raw = load_adult()\n",
        "    print(\"Loaded data shape:\", X_raw.shape, getattr(y_raw, \"shape\", None))\n",
        "\n",
        "    # quick cleaning: replace '?' with NaN, then drop rows with missing values\n",
        "    X = X_raw.copy().reset_index(drop=True)\n",
        "    y = y_raw.copy().reset_index(drop=True)\n",
        "    X = X.replace('?', np.nan)\n",
        "    missing_before = X.isna().sum().sum()\n",
        "    print(\"Missing values in features (total):\", missing_before)\n",
        "    X = X.dropna().reset_index(drop=True)\n",
        "    y = y.loc[X.index].reset_index(drop=True)\n",
        "    print(\"After dropping missing rows:\", X.shape)\n",
        "\n",
        "    # label encode categorical features\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
        "    print(\"Categorical cols:\", len(categorical_cols), \"Numeric cols:\", len(numeric_cols))\n",
        "    encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col].astype(str))\n",
        "        encoders[col] = le\n",
        "    target_le = LabelEncoder()\n",
        "    y = target_le.fit_transform(y.astype(str))\n",
        "\n",
        "    # Train / val / test split\n",
        "    # NOTE: assignment text is ambiguous; we choose 60/20/20. To make it 80/10/10,\n",
        "    # change the split parameters below accordingly.\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
        "    print(\"Shapes -> train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)\n",
        "\n",
        "    # convert to numpy arrays for our implementation\n",
        "    X_train_np = X_train.values\n",
        "    X_val_np = X_val.values\n",
        "    X_test_np = X_test.values\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    # Experiments\n",
        "    depths = [2,4,6,None]\n",
        "    criteria = ['gini', 'entropy']\n",
        "    results = []\n",
        "\n",
        "    for crit in criteria:\n",
        "        for depth in depths:\n",
        "            # full (no pre-pruning)\n",
        "            dt_full = DecisionTreeFromScratch(criterion=crit, max_depth=None, min_samples_split=2)\n",
        "            dt_full.fit(X_train_np, y_train)\n",
        "            res_full = evaluate_model(dt_full, X_test_np, y_test)\n",
        "            res_full['name'] = f\"Scratch-{crit}-full\"\n",
        "\n",
        "            # pre-pruned (max depth = depth, min_samples_split=5)\n",
        "            dt_pre = DecisionTreeFromScratch(criterion=crit, max_depth=depth, min_samples_split=5)\n",
        "            dt_pre.fit(X_train_np, y_train)\n",
        "            res_pre = evaluate_model(dt_pre, X_test_np, y_test)\n",
        "            res_pre['name'] = f\"Scratch-{crit}-pre_d{depth}\"\n",
        "\n",
        "            # post-pruned: grow full then prune using validation set\n",
        "            dt_post = DecisionTreeFromScratch(criterion=crit, max_depth=None, min_samples_split=2)\n",
        "            dt_post.fit(X_train_np, y_train)\n",
        "            dt_post.post_prune(X_val_np, y_val)\n",
        "            res_post = evaluate_model(dt_post, X_test_np, y_test)\n",
        "            res_post['name'] = f\"Scratch-{crit}-postpruned\"\n",
        "\n",
        "            # sklearn baseline (matching criterion and max_depth)\n",
        "            skl = SKDecisionTree(criterion='gini' if crit=='gini' else 'entropy', max_depth=depth, min_samples_split=5, random_state=42)\n",
        "            skl.fit(X_train, y_train)\n",
        "            skl_preds = skl.predict(X_test)\n",
        "            acc = accuracy_score(y_test, skl_preds)\n",
        "            prec, rec, f1, _ = precision_recall_fscore_support(y_test, skl_preds, average='binary', zero_division=0)\n",
        "            cm = confusion_matrix(y_test, skl_preds)\n",
        "            res_skl = {'name': f'Sklearn-{crit}-maxdepth_{depth}', 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'confusion_matrix': cm}\n",
        "\n",
        "            results.extend([res_full, res_pre, res_post, res_skl])\n",
        "            print(\"-- done:\", crit, \"depth\", depth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qo1hq26jr6jO"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    X_raw, y_raw = load_adult()\n",
        "    print(\"Loaded data shape:\", X_raw.shape, getattr(y_raw, \"shape\", None))\n",
        "\n",
        "    # quick cleaning: replace '?' with NaN, then drop rows with missing values\n",
        "    X = X_raw.copy().reset_index(drop=True)\n",
        "    y = y_raw.copy().reset_index(drop=True)\n",
        "    X = X.replace('?', np.nan)\n",
        "    missing_before = X.isna().sum().sum()\n",
        "    print(\"Missing values in features (total):\", missing_before)\n",
        "    X = X.dropna().reset_index(drop=True)\n",
        "    y = y.loc[X.index].reset_index(drop=True)\n",
        "    print(\"After dropping missing rows:\", X.shape)\n",
        "\n",
        "    # label encode categorical features\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
        "    print(\"Categorical cols:\", len(categorical_cols), \"Numeric cols:\", len(numeric_cols))\n",
        "    encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col].astype(str))\n",
        "        encoders[col] = le\n",
        "    target_le = LabelEncoder()\n",
        "    y = target_le.fit_transform(y.astype(str))\n",
        "\n",
        "    # Train / val / test split\n",
        "    # NOTE: assignment text is ambiguous; we choose 60/20/20. To make it 80/10/10,\n",
        "    # change the split parameters below accordingly.\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
        "    print(\"Shapes -> train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)\n",
        "\n",
        "    # convert to numpy arrays for our implementation\n",
        "    X_train_np = X_train.values\n",
        "    X_val_np = X_val.values\n",
        "    X_test_np = X_test.values\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    # Experiments\n",
        "    depths = [2,4,6,None]\n",
        "    criteria = ['gini', 'entropy']\n",
        "    results = []\n",
        "\n",
        "    for crit in criteria:\n",
        "        for depth in depths:\n",
        "            # full (no pre-pruning)\n",
        "            dt_full = DecisionTreeFromScratch(criterion=crit, max_depth=None, min_samples_split=2)\n",
        "            dt_full.fit(X_train_np, y_train)\n",
        "            res_full = evaluate_model(dt_full, X_test_np, y_test)\n",
        "            res_full['name'] = f\"Scratch-{crit}-full\"\n",
        "\n",
        "            # pre-pruned (max depth = depth, min_samples_split=5)\n",
        "            dt_pre = DecisionTreeFromScratch(criterion=crit, max_depth=depth, min_samples_split=5)\n",
        "            dt_pre.fit(X_train_np, y_train)\n",
        "            res_pre = evaluate_model(dt_pre, X_test_np, y_test)\n",
        "            res_pre['name'] = f\"Scratch-{crit}-pre_d{depth}\"\n",
        "\n",
        "            # post-pruned: grow full then prune using validation set\n",
        "            dt_post = DecisionTreeFromScratch(criterion=crit, max_depth=None, min_samples_split=2)\n",
        "            dt_post.fit(X_train_np, y_train)\n",
        "            dt_post.post_prune(X_val_np, y_val)\n",
        "            res_post = evaluate_model(dt_post, X_test_np, y_test)\n",
        "            res_post['name'] = f\"Scratch-{crit}-postpruned\"\n",
        "\n",
        "            # sklearn baseline (matching criterion and max_depth)\n",
        "            skl = SKDecisionTree(criterion='gini' if crit=='gini' else 'entropy', max_depth=depth, min_samples_split=5, random_state=42)\n",
        "            skl.fit(X_train, y_train)\n",
        "            skl_preds = skl.predict(X_test)\n",
        "            acc = accuracy_score(y_test, skl_preds)\n",
        "            prec, rec, f1, _ = precision_recall_fscore_support(y_test, skl_preds, average='binary', zero_division=0)\n",
        "            cm = confusion_matrix(y_test, skl_preds)\n",
        "            res_skl = {'name': f'Sklearn-{crit}-maxdepth_{depth}', 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'confusion_matrix': cm}\n",
        "\n",
        "            results.extend([res_full, res_pre, res_post, res_skl])\n",
        "            print(\"-- done:\", crit, \"depth\", depth)\n",
        "\n",
        "    summary = []\n",
        "    for r in results:\n",
        "        summary.append({'Model': r['name'], 'Accuracy': round(r['accuracy'],4), 'Precision': round(r['precision'],4), 'Recall': round(r['recall'],4), 'F1': round(r['f1'],4)})\n",
        "    summary_df = pd.DataFrame(summary)\n",
        "    print(\"\\nSUMMARY:\")\n",
        "    print(summary_df.sort_values('Accuracy', ascending=False).reset_index(drop=True))\n",
        "\n",
        "    # Root feature from last dt_full (gini or entropy depending loop order)\n",
        "    try:\n",
        "        root_info = (feature_names[dt_full.root.feature_index], dt_full.root.threshold, dt_full.root.is_categorical)\n",
        "        print(\"\\nRoot feature used by last full scratch tree:\", root_info)\n",
        "    except Exception:\n",
        "        print(\"Could not extract root info.\")\n",
        "\n",
        "    # show sklearn feature importances\n",
        "    skl_full = SKDecisionTree(criterion='gini', random_state=42)\n",
        "    skl_full.fit(X_train, y_train)\n",
        "    feat_imp = pd.Series(skl_full.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
        "    print(\"\\nTop sklearn feature importances:\\n\", feat_imp.head(10))\n",
        "\n",
        "    # Save outputs\n",
        "    out_dir = \"./decision_tree_outputs\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    joblib.dump(dt_full, os.path.join(out_dir, 'dt_full_scratch.joblib'))\n",
        "    joblib.dump(dt_post, os.path.join(out_dir, 'dt_post_scratch.joblib'))\n",
        "    joblib.dump(skl_full, os.path.join(out_dir, 'skl_full.joblib'))\n",
        "    joblib.dump(encoders, os.path.join(out_dir, 'encoders.joblib'))\n",
        "    joblib.dump(target_le, os.path.join(out_dir, 'target_le.joblib'))\n",
        "    summary_df.to_csv(os.path.join(out_dir, 'experiment_summary.csv'), index=False)\n",
        "    print(\"\\nSaved models & results to:\", out_dir)\n",
        "\n",
        "    # Print classification report for best model by accuracy\n",
        "    best = max(results, key=lambda x: x['accuracy'])\n",
        "    print(\"\\nBest model on test:\", best['name'], \"Accuracy:\", best['accuracy'])\n",
        "    # print confusion matrix & classification report\n",
        "    print(\"Confusion matrix:\\n\", best['confusion_matrix'])\n",
        "    # get classification report\n",
        "    # For the best model, pick the model instance accordingly (approx)\n",
        "    if best['name'].startswith('Scratch') and 'post' in best['name']:\n",
        "        report_preds = dt_post.predict(X_test_np)\n",
        "    elif best['name'].startswith('Scratch') and 'pre' in best['name']:\n",
        "        report_preds = dt_pre.predict(X_test_np)\n",
        "    elif best['name'].startswith('Scratch') and 'full' in best['name']:\n",
        "        report_preds = dt_full.predict(X_test_np)\n",
        "    else:\n",
        "        report_preds = skl_full.predict(X_test)\n",
        "    print(\"\\nClassification report (best model):\\n\", classification_report(y_test, report_preds, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrbNWSFfsArZ",
        "outputId": "9dd682a1-04dd-4313-898c-87b2cb672830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ucimlrepo unavailable or fetch failed: No module named 'ucimlrepo'\n",
            "Loaded data shape: (48842, 14) (48842,)\n",
            "Missing values in features (total): 6465\n",
            "After dropping missing rows: (45222, 14)\n",
            "Categorical cols: 8 Numeric cols: 6\n",
            "Shapes -> train: (27132, 14) val: (9045, 14) test: (9045, 14)\n",
            "-- done: gini depth 2\n",
            "-- done: gini depth 4\n",
            "-- done: gini depth 6\n",
            "-- done: gini depth None\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1lmgLApsP19"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}